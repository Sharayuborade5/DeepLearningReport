{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AFN1Up6i8v3f",
        "outputId": "96fc466c-ec01-429c-e886-d20b1f407a9b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " flatten (Flatten)           (None, 784)               0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 128)               100480    \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 128)               16512     \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 10)                1290      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 118282 (462.04 KB)\n",
            "Trainable params: 118282 (462.04 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "#Q3\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "\n",
        "# Define the model architecture\n",
        "def create_dense_nn(input_shape, num_classes, hidden_layers=2, neurons_per_layer=128, activation='relu'):\n",
        "    model = models.Sequential()\n",
        "    model.add(layers.Flatten(input_shape=input_shape))  # Flatten the input images\n",
        "    for _ in range(hidden_layers):\n",
        "        model.add(layers.Dense(neurons_per_layer, activation=activation))\n",
        "    model.add(layers.Dense(num_classes, activation='softmax'))\n",
        "    return model\n",
        "\n",
        "# Example usage\n",
        "input_shape = (28, 28)  # Assuming input images are 28x28 pixels\n",
        "num_classes = 10  # Number of output classes\n",
        "model = create_dense_nn(input_shape, num_classes)\n",
        "model.summary()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Q4\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "from torchvision import datasets, transforms, models\n",
        "from torch.utils.data import DataLoader, SubsetRandomSampler\n",
        "import numpy as np\n",
        "\n",
        "# Define data transformations and download the SVHN dataset\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((32, 32)),  # Resize images to 32x32\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "# Download the SVHN dataset\n",
        "train_dataset = datasets.SVHN(root='./data', split='train', download=True, transform=transform)\n",
        "\n",
        "# Create a subset of the dataset (25%)\n",
        "subset_indices = np.random.choice(len(train_dataset), int(0.25 * len(train_dataset)), replace=False)\n",
        "subset_sampler = SubsetRandomSampler(subset_indices)\n",
        "\n",
        "# Create data loaders for training and validation\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, sampler=subset_sampler)\n",
        "val_loader = DataLoader(train_dataset, batch_size=64, sampler=subset_sampler)\n",
        "\n",
        "# Define LeNet-5 architecture with corrected pooling sizes\n",
        "class LeNet5(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(LeNet5, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
        "        self.pool1 = nn.MaxPool2d(2, 2)\n",
        "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
        "        self.pool2 = nn.MaxPool2d(2, 2)\n",
        "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
        "        self.fc2 = nn.Linear(120, 84)\n",
        "        self.fc3 = nn.Linear(84, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool1(torch.relu(self.conv1(x)))\n",
        "        x = self.pool2(torch.relu(self.conv2(x)))\n",
        "        x = torch.flatten(x, 1)  # Flatten the output for fully connected layers\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "# Define training and evaluation functions\n",
        "def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=10):\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        for images, labels in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        # Calculate validation loss\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        with torch.no_grad():\n",
        "            for images, labels in val_loader:\n",
        "                outputs = model(images)\n",
        "                val_loss += criterion(outputs, labels).item()\n",
        "\n",
        "        print(f'Epoch {epoch+1}/{num_epochs}, Training Loss: {running_loss/len(train_loader)}, Validation Loss: {val_loss/len(val_loader)}')\n",
        "\n",
        "# Instantiate LeNet-5 model\n",
        "model = LeNet5()\n",
        "\n",
        "# Define loss function, optimizer, and train the model\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "train_model(model, train_loader, val_loader, criterion, optimizer)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D-ZeaB8l9E3L",
        "outputId": "480a01a0-8125-4ab1-e729-7ce44edf6f5f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://ufldl.stanford.edu/housenumbers/train_32x32.mat to ./data/train_32x32.mat\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 182040794/182040794 [00:09<00:00, 19229897.61it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10, Training Loss: 1.6961113424251304, Validation Loss: 1.076909353093403\n",
            "Epoch 2/10, Training Loss: 0.8316068453032796, Validation Loss: 0.6760756693235258\n",
            "Epoch 3/10, Training Loss: 0.6169246307441166, Validation Loss: 0.533748140629991\n",
            "Epoch 4/10, Training Loss: 0.5206320909880595, Validation Loss: 0.45585297712673295\n",
            "Epoch 5/10, Training Loss: 0.46379317223818045, Validation Loss: 0.40897976786209733\n",
            "Epoch 6/10, Training Loss: 0.4161881695433361, Validation Loss: 0.3530190512537956\n",
            "Epoch 7/10, Training Loss: 0.37726627456394224, Validation Loss: 0.3402009753815388\n",
            "Epoch 8/10, Training Loss: 0.3435512104100882, Validation Loss: 0.31827276020930617\n",
            "Epoch 9/10, Training Loss: 0.31472860276699066, Validation Loss: 0.2831742942385142\n",
            "Epoch 10/10, Training Loss: 0.2894549837874619, Validation Loss: 0.24076448772946302\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "r6Gyi3lxEHRg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}